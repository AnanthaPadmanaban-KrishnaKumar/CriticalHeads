{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Determine and return the available computation device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "def load_sam_model(model_type, checkpoint_path, device):\n",
    "    \"\"\"Load the SAM model and move it to the specified device.\"\"\"\n",
    "    print(f\"Loading SAM model ({model_type})...\")\n",
    "    model = sam_model_registry[model_type](checkpoint=checkpoint_path)\n",
    "    model.to(device=device)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    return model\n",
    "\n",
    "def calculate_model_size(model, checkpoint_path=None):\n",
    "    \"\"\"Calculate and return the model size information.\"\"\"\n",
    "    # Calculate model size in memory\n",
    "    model_size_bytes = 0\n",
    "    for param in model.parameters():\n",
    "        model_size_bytes += param.nelement() * param.element_size()\n",
    "    \n",
    "    model_size_mb = model_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    # Calculate checkpoint file size if provided\n",
    "    checkpoint_size_mb = 0\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint_size_mb = os.path.getsize(checkpoint_path) / (1024 * 1024)\n",
    "    \n",
    "    return {\n",
    "        \"model_size_mb\": model_size_mb,\n",
    "        \"checkpoint_size_mb\": checkpoint_size_mb\n",
    "    }\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count and return parameter statistics for the model.\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "    \n",
    "    return {\n",
    "        \"total\": total_params,\n",
    "        \"trainable\": trainable_params,\n",
    "        \"non_trainable\": non_trainable_params\n",
    "    }\n",
    "\n",
    "def analyze_top_level_components(model):\n",
    "    \"\"\"Analyze the top-level components of the model.\"\"\"\n",
    "    components = []\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    for name, module in model.named_children():\n",
    "        num_params = sum(p.numel() for p in module.parameters())\n",
    "        percent_of_model = (num_params / total_params * 100) if total_params > 0 else 0\n",
    "        \n",
    "        components.append({\n",
    "            \"name\": name,\n",
    "            \"type\": module.__class__.__name__,\n",
    "            \"parameters\": num_params,\n",
    "            \"percent_of_model\": percent_of_model\n",
    "        })\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Define attention analysis functions\n",
    "def analyze_attention_mechanisms(model):\n",
    "    \"\"\"Recursively analyze attention mechanisms in the model.\"\"\"\n",
    "    attention_info = {}\n",
    "    \n",
    "    def _analyze_attention(module, prefix=\"\"):\n",
    "        module_name = prefix\n",
    "        if module_name not in attention_info and hasattr(module, \"num_heads\"):\n",
    "            attention_info[module_name] = {\n",
    "                \"num_heads\": module.num_heads,\n",
    "                \"head_dim\": getattr(module, \"head_dim\", None),\n",
    "                \"embedding_dim\": getattr(module, \"embedding_dim\", None)\n",
    "            }\n",
    "        \n",
    "        # Recursively analyze children\n",
    "        for name, child in module.named_children():\n",
    "            child_name = f\"{prefix}.{name}\" if prefix else name\n",
    "            _analyze_attention(child, child_name)\n",
    "    \n",
    "    # Start recursive analysis\n",
    "    _analyze_attention(model)\n",
    "    return attention_info\n",
    "\n",
    "def analyze_image_encoder_attention(model):\n",
    "    \"\"\"Analyze the attention mechanisms in the image encoder.\"\"\"\n",
    "    if not hasattr(model, \"image_encoder\"):\n",
    "        return {\"has_image_encoder\": False}\n",
    "    \n",
    "    vit = model.image_encoder\n",
    "    result = {\"has_image_encoder\": True}\n",
    "    \n",
    "    # Analyze transformer blocks\n",
    "    if hasattr(vit, \"blocks\"):\n",
    "        blocks_info = []\n",
    "        num_layers = len(vit.blocks)\n",
    "        result[\"num_layers\"] = num_layers\n",
    "        \n",
    "        # Check first block for attention details\n",
    "        if num_layers > 0:\n",
    "            first_block = vit.blocks[0]\n",
    "            if hasattr(first_block, \"attn\"):\n",
    "                attn = first_block.attn\n",
    "                num_heads = getattr(attn, \"num_heads\", None)\n",
    "                head_dim = getattr(attn, \"head_dim\", None)\n",
    "                \n",
    "                result[\"attention_heads_per_layer\"] = num_heads\n",
    "                result[\"head_dimension\"] = head_dim\n",
    "                \n",
    "                if isinstance(num_heads, int) and isinstance(head_dim, int):\n",
    "                    result[\"attention_capacity_per_layer\"] = num_heads * head_dim\n",
    "        \n",
    "        # Analyze each block's attention\n",
    "        for i, block in enumerate(vit.blocks):\n",
    "            if hasattr(block, \"attn\") and hasattr(block.attn, \"num_heads\"):\n",
    "                blocks_info.append({\n",
    "                    \"block_index\": i,\n",
    "                    \"num_heads\": block.attn.num_heads\n",
    "                })\n",
    "        \n",
    "        result[\"blocks\"] = blocks_info\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading SAM model (vit_b)...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model_type = \"vit_b\"\n",
    "checkpoint_path = \"/Users/haki911/Documents/research/segment-anything/checkpoint/sam_vit_b_01ec64.pth\"\n",
    "\n",
    "# Get device and load model\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = load_sam_model(model_type, checkpoint_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze model size and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size in memory: 357.57 MB\n",
      "Checkpoint file size: 357.67 MB\n",
      "\n",
      "Total parameters: 93,735,472\n"
     ]
    }
   ],
   "source": [
    "# Calculate model size\n",
    "size_info = calculate_model_size(model, checkpoint_path)\n",
    "print(f\"Model size in memory: {size_info['model_size_mb']:.2f} MB\")\n",
    "print(f\"Checkpoint file size: {size_info['checkpoint_size_mb']:.2f} MB\")\n",
    "print(\"\")\n",
    "# Count parameters\n",
    "param_info = count_parameters(model)\n",
    "print(f\"Total parameters: {param_info['total']:,}\")\n",
    "# print(f\"Trainable parameters: {param_info['trainable']:,}\")\n",
    "# print(f\"Non-trainable parameters: {param_info['non_trainable']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze top-level components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== TOP-LEVEL COMPONENTS ===============\n",
      "- image_encoder: ImageEncoderViT\n",
      "  Parameters: 89,670,912 (95.66% of model)\n",
      "- prompt_encoder: PromptEncoder\n",
      "  Parameters: 6,220 (0.01% of model)\n",
      "- mask_decoder: MaskDecoder\n",
      "  Parameters: 4,058,340 (4.33% of model)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*15} TOP-LEVEL COMPONENTS {'='*15}\")\n",
    "components = analyze_top_level_components(model)\n",
    "for comp in components:\n",
    "    print(f\"- {comp['name']}: {comp['type']}\")\n",
    "    print(f\"  Parameters: {comp['parameters']:,} ({comp['percent_of_model']:.2f}% of model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Encoder Attention Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== IMAGE ENCODER ATTENTION ===============\n",
      "Image Encoder Attention Analysis:\n",
      "Number of transformer layers: 12\n",
      "Attention heads per layer: 12\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: \n",
    "print(f\"{'='*15} IMAGE ENCODER ATTENTION {'='*15}\")\n",
    "img_encoder_info = analyze_image_encoder_attention(model)\n",
    "\n",
    "if img_encoder_info.get(\"has_image_encoder\", False):\n",
    "    print(\"Image Encoder Attention Analysis:\")\n",
    "    print(f\"Number of transformer layers: {img_encoder_info.get('num_layers', 'Unknown')}\")\n",
    "    print(f\"Attention heads per layer: {img_encoder_info.get('attention_heads_per_layer', 'Unknown')}\")\n",
    "else:\n",
    "    print(\"The model does not have an image encoder component.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== ALL ATTENTION MECHANISMS ===============\n",
      "Components with Attention Mechanisms:\n",
      "  - image_encoder.blocks.0.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.1.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.2.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.3.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.4.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.5.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.6.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.7.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.8.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.9.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.10.attn:\n",
      "    Number of heads: 12\n",
      "  - image_encoder.blocks.11.attn:\n",
      "    Number of heads: 12\n",
      "  - mask_decoder.transformer:\n",
      "    Number of heads: 8\n",
      "    Embedding dimension: 256\n",
      "  - mask_decoder.transformer.layers.0.self_attn:\n",
      "    Number of heads: 8\n",
      "    Embedding dimension: 256\n",
      "  - mask_decoder.transformer.layers.0.cross_attn_token_to_image:\n",
      "    Number of heads: 8\n",
      "    Embedding dimension: 256\n",
      "  - mask_decoder.transformer.layers.0.cross_attn_image_to_token:\n",
      "    Number of heads: 8\n",
      "    Embedding dimension: 256\n",
      "  - mask_decoder.transformer.layers.1.self_attn:\n",
      "    Number of heads: 8\n",
      "    Embedding dimension: 256\n",
      "  - mask_decoder.transformer.layers.1.cross_attn_token_to_image:\n",
      "    Number of heads: 8\n",
      "    Embedding dimension: 256\n",
      "  - mask_decoder.transformer.layers.1.cross_attn_image_to_token:\n",
      "    Number of heads: 8\n",
      "    Embedding dimension: 256\n",
      "  - mask_decoder.transformer.final_attn_token_to_image:\n",
      "    Number of heads: 8\n",
      "    Embedding dimension: 256\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*15} ALL ATTENTION MECHANISMS {'='*15}\")\n",
    "attention_info = analyze_attention_mechanisms(model)\n",
    "\n",
    "print(\"Components with Attention Mechanisms:\")\n",
    "for name, info in attention_info.items():\n",
    "    heads = info[\"num_heads\"]\n",
    "    head_dim = info[\"head_dim\"]\n",
    "    emb_dim = info[\"embedding_dim\"]\n",
    "    \n",
    "    print(f\"  - {name}:\")\n",
    "    print(f\"    Number of heads: {heads}\")\n",
    "    if head_dim:\n",
    "        print(f\"    Head dimension: {head_dim}\")\n",
    "    if emb_dim:\n",
    "        print(f\"    Embedding dimension: {emb_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== COMPLETE MODEL ARCHITECTURE ===============\n",
      "Sam(\n",
      "  (image_encoder): ImageEncoderViT(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (neck): Sequential(\n",
      "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): LayerNorm2d()\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (3): LayerNorm2d()\n",
      "    )\n",
      "  )\n",
      "  (prompt_encoder): PromptEncoder(\n",
      "    (pe_layer): PositionEmbeddingRandom()\n",
      "    (point_embeddings): ModuleList(\n",
      "      (0-3): 4 x Embedding(1, 256)\n",
      "    )\n",
      "    (not_a_point_embed): Embedding(1, 256)\n",
      "    (mask_downscaling): Sequential(\n",
      "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): LayerNorm2d()\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (no_mask_embed): Embedding(1, 256)\n",
      "  )\n",
      "  (mask_decoder): MaskDecoder(\n",
      "    (transformer): TwoWayTransformer(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TwoWayAttentionBlock(\n",
      "          (self_attn): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_token_to_image): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): MLPBlock(\n",
      "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "            (act): ReLU()\n",
      "          )\n",
      "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (cross_attn_image_to_token): Attention(\n",
      "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_attn_token_to_image): Attention(\n",
      "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "      )\n",
      "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (iou_token): Embedding(1, 256)\n",
      "    (mask_tokens): Embedding(4, 256)\n",
      "    (output_upscaling): Sequential(\n",
      "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (1): LayerNorm2d()\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (4): GELU(approximate='none')\n",
      "    )\n",
      "    (output_hypernetworks_mlps): ModuleList(\n",
      "      (0-3): 4 x MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (iou_prediction_head): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*15} COMPLETE MODEL ARCHITECTURE {'='*15}\")\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
